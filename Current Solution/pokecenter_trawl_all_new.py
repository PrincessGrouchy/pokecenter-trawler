# Generated by Selenium IDE
import time
import pandas as pd
import warnings

import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support import expected_conditions
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.firefox.service import Service as FirefoxService
from selenium.webdriver.ie.service import Service as IEService

from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.firefox import GeckoDriverManager
from webdriver_manager.microsoft import EdgeChromiumDriverManager
from webdriver_manager.microsoft import IEDriverManager

from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.edge.options import Options as EdgeOptions
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.ie.options import Options as IEOptions

from selenium.common.exceptions import NoSuchElementException


class EnTrawler():
    def pre_setup_vars(self):
        # ser = IEService(
        #     "C:\\Users\\arebe\\OneDrive\\OneDocs\\selenium_drivers\\IEDriverServer.exe")
        op = IEOptions()
        op.attach_to_edge_chrome = True
        op.ignore_zoom_level = True
        op.ignore_protected_mode_settings = True
        op.initial_browser_url = "https://selenium.dev"
        op.edge_executable_path = "C:/Program Files (x86)/Microsoft/Edge/Application/msedge.exe"
        self.driver = webdriver.Ie(options=op)

        # self.service = IEService(executable_path=IEDriverManager().install())
        # self.driver = webdriver.Ie(service=self.service)
        self.driver.implicitly_wait(120)
        self.driver.set_window_size(776, 720)

    def setup_method(self, the_region, the_category):
        self.region = the_region
        self.category = the_category
        self.sort = "?sort=launch_date%2Bdesc"
        self.num_items_displayed = 90  # 30 # 60

        self.url = "https://www.pokemoncenter.com/" + self.region + \
            "/category" + self.category + self.sort + \
            "&ps=" + str(self.num_items_displayed) + "&page="
        self.new_file = []
        self.new_file_name = 'pokecenter_output_' + \
            self.region + '_new.txt'

        # warnings.simplefilter('ignore', FutureWarning)

    def teardown_method(self):
        self.driver.quit()
        self.new_file_pd = pd.DataFrame(self.new_file,
                                        columns=['relative_position',
                                                 'link',
                                                 'id',
                                                 'name',
                                                 'price',
                                                 'img_link',
                                                 'in_stock',
                                                 'page_position',
                                                 'category'],
                                        )

        self.new_file_pd.to_csv(self.new_file_name, index=False)

    def print_line(self):
        if self.stock == "SOLD OUT" or "soldout" in self.stock:
            in_stock = "NO"
        else:
            in_stock = "YES"
        product_number = self.link.replace("https://www.pokemoncenter.com/"
                                           + self.region
                                           + "/product/", "")
        product_number = product_number.replace(
            "https://www.pokemoncenter.com/product/", "")
        product_number = product_number.replace(
            "https://www.pokemoncenter-online.com/?p_cd=", "")
        product_number = product_number.split("/")[0]

        comma_stripped_name = self.name.replace(
            ",", "%2C")  # comma is %2C in url encoding
        comma_stripped_name = comma_stripped_name.replace("New! ", "")
        comma_stripped_price = self.price.replace(
            ",", "")
        stripped_category = self.category.replace("/", "")

        real_number = self.page_count - 1
        real_number *= self.num_items_displayed
        real_number += self.loop_count

        row_to_add = [
            real_number,
            self.link,
            product_number,
            comma_stripped_name,
            self.img_link,
            comma_stripped_price,
            in_stock,
            str(self.page_count) + "-" + str(self.loop_count),
            stripped_category
        ]
        self.new_file.append(row_to_add)

    def get_page_vars_old(self, isFirst):
        self.base_xpath = "//main[@id='main']/div[2]/div[2]/div[2]/div[4]/"
        if isFirst:
            extra_div = "div"
        else:
            extra_div = "div[" + str(self.loop_count) + "]"
        self.link = self.driver.find_element(By.XPATH,
                                             self.base_xpath + extra_div + "/div/a").get_attribute("href")
        self.name = self.driver.find_element(By.XPATH,
                                             self.base_xpath + extra_div + "/div/a/div[2]/h3").text
        self.price = self.driver.find_element(By.XPATH,
                                              self.base_xpath + extra_div + "/div/a/div[2]/div/span").text
        self.stock = self.driver.find_element(By.XPATH,
                                              self.base_xpath + extra_div + "/div/a/div/div").text

    def get_page_vars(self, isFirst):
        self.base_css = "main#main > div:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(4)"
        # if isFirst:
        #     extra_div = " > div"
        # else:
        extra_div = " > div:nth-of-type(" + str(self.loop_count) + ")"
        self.link = self.driver.find_element(By.CSS_SELECTOR,
                                             self.base_css + extra_div + " > div > a").get_attribute("href")
        self.name = self.driver.find_element(By.CSS_SELECTOR,
                                             self.base_css + extra_div + " > div > a > div:nth-of-type(1) > h3").text
        self.price = self.driver.find_element(By.CSS_SELECTOR,
                                              self.base_css + extra_div + " > div > a > div:nth-of-type(1) > div > span").text
        self.img_link = self.driver.find_element(By.CSS_SELECTOR,
                                                 self.base_css + extra_div + " > div > div > a > img:nth-of-type(1)").get_attribute("src")
        self.driver.implicitly_wait(0)
        try:
            self.stock = self.driver.find_element(By.CSS_SELECTOR,
                                                  self.base_css + extra_div + " > div > div > a > div").text
        except NoSuchElementException:
            self.stock = ""
        self.driver.implicitly_wait(120)

    def scrape_page_not_last_page(self):
        self.page_count = 1
        self.loop_count = 1

        self.driver.get(self.url + str(self.page_count))

        # WebDriverWait(self.driver, 120).until(
        #     lambda driver: '|' in driver.title)

        # self.total_count_string = self.driver.find_element(By.XPATH,
        #                                                    "//main[@id='main']/div[2]/div[2]/div[2]/div[3]/div/div/h3/span").text
        self.total_count_string = self.driver.find_element(By.CSS_SELECTOR,
                                                           "main#main > div:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(3) > div > div > h3 > span").text
        self.total_count = self.total_count_string.split("of ")[
            1].replace(" )", "")
        self.total_full_page_count = int(
            self.total_count) // int(self.num_items_displayed)
        self.total_page_count = self.total_full_page_count + 1

        while self.page_count < self.total_page_count:
            self.loop_count = 1
            self.driver.get(self.url + str(self.page_count))
            self.get_page_vars(isFirst=True)
            self.print_line()

            self.loop_count = 2
            while self.loop_count < (self.num_items_displayed + 1):
                self.get_page_vars(isFirst=False)
                self.print_line()
                self.loop_count += 1

            self.page_count += 1
            print(self.page_count)

    def scrape_page_last(self):
        # for last page only runs
        # self.total_count = 1079 # set manually!
        # self.total_full_page_count = int(self.total_count) // int(self.num_items_displayed)
        # self.total_page_count = self.total_full_page_count + 1

        self.page_count = self.total_page_count
        self.loop_count = 1
        # problem: we need to skip if the page remainder is 0!
        if int(self.total_count) % int(self.num_items_displayed) != 0:
            self.driver.get(self.url + str(self.page_count))
            self.get_page_vars(isFirst=True)
            self.print_line()

            self.loop_count = 2
            self.past_items = int(self.total_full_page_count) * \
                int(self.num_items_displayed)
            self.new_num_items = int(self.total_count) - int(self.past_items)
            while self.loop_count < (self.new_num_items + 1):
                self.get_page_vars(isFirst=False)
                self.print_line()
                self.loop_count += 1


class JpTrawler(EnTrawler):
    def pre_setup_vars(self):
        self.service = FirefoxService(
            executable_path=GeckoDriverManager().install())
        self.driver = webdriver.Firefox(service=self.service)
        self.driver.implicitly_wait(120)
        self.driver.set_window_size(776.642, 701.800)

    def setup_method(self, the_category):
        self.region = "jp-jp"
        self.sort = "&sort=new"
        self.category = the_category  # plush
        self.url = "https://www.pokemoncenter-online.com/?main_page=product_list" + \
            self.sort + "&cat1=" + self.category + "&page="
        self.translate_url_additions = ""  # chrome autotranslate adds "/font/font"
        self.num_items_displayed = 40  # default=40

        self.new_file_name = 'pokecenter_output_' + \
            self.region + '_new.txt'
        self.new_file = []

    def get_page_vars(self, isFirst):
        self.base_xpath = "//div[@id='product_list']/ul/"
        if isFirst:
            extra_div = "li"
        else:
            extra_div = "li[" + str(self.loop_count) + "]"
        # xpath=//div[@id='product_list']/ul/li/a
        self.link = self.driver.find_element(By.XPATH,
                                             self.base_xpath + extra_div + "/a").get_attribute("href")
        #  xpath=//div[@id='product_list']/ul/li[2]/a/div/p/font/font
        self.name = self.driver.find_element(By.XPATH,
                                             self.base_xpath + extra_div + "/a/div/p" + self.translate_url_additions).text
        # xpath=//div[@id='product_list']/ul/li[2]/a/div/p[2]/font/font
        self.price = self.driver.find_element(By.XPATH,
                                              self.base_xpath + extra_div + "/a/div/p[2]" + self.translate_url_additions).text
        # xpath=//div[@id='product_list']/ul/li[4]
        self.stock = self.driver.find_element(By.XPATH,
                                              self.base_xpath + extra_div).get_attribute("class")
        self.img_link = self.driver.find_element(By.XPATH,
                                                 self.base_xpath + extra_div + "/a/img").get_attribute("src")

    def scrape_page_not_last_page(self):
        self.page_count = 1
        self.loop_count = 1
        self.driver.get(self.url + str(self.page_count))
        WebDriverWait(self.driver, 120).until(
            lambda driver: 'ポケモンセンターオンライン' in driver.title)

        self.total_count_string = self.driver.find_element(By.XPATH,
                                                           "//div[@id='contents']/div/h1" + self.translate_url_additions).text
        self.total_count = self.total_count_string.split("全 ")[
            1].replace("件）", "").replace(",", "")
        self.total_full_page_count = int(
            self.total_count) // int(self.num_items_displayed)
        self.total_page_count = self.total_full_page_count + 1

        while self.page_count < self.total_page_count:
            self.loop_count = 1
            self.driver.get(self.url + str(self.page_count))
            self.get_page_vars(isFirst=True)
            self.print_line()

            self.loop_count = 2
            while self.loop_count < (self.num_items_displayed + 1):
                self.get_page_vars(isFirst=False)
                self.print_line()
                self.loop_count += 1

            self.page_count += 1


categories = ["/new-releases", "/plush", "/figures-and-pins",
              "/trading-card-game", "/clothing", "/home", "/video-game"]
regions = ["en-ca", "en-gb", "en-us"]
en = EnTrawler()
en.pre_setup_vars()

for the_region in regions:
     for the_category in categories:
         print(the_region + " " + the_category)
         en.setup_method(the_region, the_category)
         en.scrape_page_not_last_page()
         en.scrape_page_last()

en.teardown_method()


jp_categories = ["toy", "game", "card",
                 "stationery", "goods",
                 "kitchen", "phone", "fashion",
                 "apparel", "book", "cddvd", "food"]  # "ticket"
jp = JpTrawler()
jp.pre_setup_vars()

for the_jp_category in jp_categories:
    print(the_jp_category)
    jp.setup_method(the_jp_category)
    jp.scrape_page_not_last_page()
    jp.scrape_page_last()

jp.teardown_method()
